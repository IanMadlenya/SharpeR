%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Asymptotic Distribution of the Markowitz Portfolio}
%\VignetteKeyword{Finance}
%\VignetteKeyword{Sharpe}
%\VignettePackage{SharpeR}
\documentclass[10pt,a4paper,english]{article}

% front matter%FOLDUP
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}
% for therefore
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[square,numbers]{natbib}
%\usepackage[authoryear]{natbib}
%\usepackage[iso]{datetime}
%\usepackage{datetime}

%compactitem and such:
\usepackage[newitem,newenum,increaseonly]{paralist}

\makeatletter
\makeatother

%\input{sr_defs.tex}
\usepackage{SharpeR}

\providecommand{\sideWarning}[1][0.5]{\marginpar{\hfill\includegraphics[width=#1\marginparwidth]{warning}}}

% knitr setup%FOLDUP

<<'preamble', include=FALSE, warning=FALSE, message=FALSE>>=
library(knitr)

# set the knitr options ... for everyone!
# if you unset this, then vignette build bonks. oh, joy.
#opts_knit$set(progress=TRUE)
opts_knit$set(eval.after='fig.cap')
# for a package vignette, you do want to echo.
# opts_chunk$set(echo=FALSE,warning=FALSE,message=FALSE)
opts_chunk$set(warning=FALSE,message=FALSE)
#opts_chunk$set(results="asis")
opts_chunk$set(cache=TRUE,cache.path="cache/SharpeRatio")

#opts_chunk$set(fig.path="figure/",dev=c("pdf","cairo_ps"))
opts_chunk$set(fig.path="figure/SharpeRatio",dev=c("pdf"))
opts_chunk$set(fig.width=5,fig.height=4,dpi=64)

# doing this means that png files are made of figures;
# the savings is small, and it looks like shit:
#opts_chunk$set(fig.path="figure/",dev=c("png","pdf","cairo_ps"))
#opts_chunk$set(fig.width=4,fig.height=4)
# for figures? this is sweave-specific?
#opts_knit$set(eps=TRUE)

# this would be for figures:
#opts_chunk$set(out.width='.8\\textwidth')
# for text wrapping:
options(width=64,digits=2)
opts_chunk$set(size="small")
opts_chunk$set(tidy=TRUE,tidy.opts=list(width.cutoff=50,keep.blank.line=TRUE))

compile.time <- Sys.time()

# from the environment

# only recompute if FORCE_RECOMPUTE=True w/out case match.
FORCE_RECOMPUTE <- 
	(toupper(Sys.getenv('FORCE_RECOMPUTE',unset='False')) == "TRUE")

# compiler flags!

# not used yet
LONG.FORM <- FALSE

mc.resolution <- ifelse(LONG.FORM,1000,200)
mc.resolution <- max(mc.resolution,100)

library(quantmod)
options("getSymbols.warning4.0"=FALSE)

library(SharpeR)

gen_norm <- rnorm
lseq <- function(from,to,length.out) { 
	exp(seq(log(from),log(to),length.out = length.out))
}
@
%UNFOLD
%UNFOLD

% document incantations%FOLDUP
\begin{document}

\title{Asymptotic Distribution of the Markowitz Portfolio}
\author{Steven E. Pav \thanks{\email{shabbychef@gmail.com}}}
%\date{\today, \currenttime}

\maketitle
%UNFOLD

\begin{abstract}%FOLDUP
The asymptotic distribution of the Markowitz portfolio, \minvAB{\svsig}{\svmu},
is derived, for the general case (assuming fourth moments of returns exist), 
and for the case of multivariate normal returns. 
The derivation allows for inference which is robust to heteroskedasticity 
and autocorrelation of moments up to order four. As a side effect, one 
can estimate the proportion of error in the Markowitz portfolio due to 
mis-estimation of the covariance matrix.
\end{abstract}%UNFOLD

\section{The Markowitz Portfolio}

\section{Multivariate inference in unified form}%FOLDUP

Let \vreti be an array of returns of \nlatf assets, with mean \pvmu, and
covariance \pvsig.  Let \avreti be \vreti prepended with a 1:
$\avreti = \asvec{1,\tr{\vreti}}$. Consider the second
moment of \avreti:
\begin{equation}
\pvsm \defeq \E{\ogram{\avreti}} = 
	\twobytwo{1}{\tr{\pvmu}}{\pvmu}{\pvsig + \ogram{\pvmu}}.
\label{eqn:pvsm_def}
\end{equation}
By inspection one can confirm that the
inverse of \pvsm is
$$
\minv{\pvsm} 
= \twobytwo{1 + \qiform{\pvsig}{\pvmu}}{-\tr{\pvmu}\minv{\pvsig}}{-\minv{\pvsig}\pvmu}{\minv{\pvsig}}
= \twobytwo{1 + \psnrsqopt}{-\tr{\pportwopt}}{-\pportwopt}{\minv{\pvsig}},
$$
where $\pportwopt$ is the Markowitz portfolio, and \psnropt is the \txtSR of
that portfolio.

The relationships above are merely facts of linear algebra, and so
hold for the sample estimates as well:
\begin{equation*}
\minv{\twobytwo{1}{\tr{\svmu}}{\svmu}{\svsig + \ogram{\svmu}}}
= {\twobytwo{1 +
\ssrsqopt}{-\tr{\sportwopt}}{-\sportwopt}{\minv{\svsig}}},
\end{equation*}
where \svmu, \svsig are some sample estimates of \pvmu and \pvsig, and 
$\sportwopt = \minvAB{\svsig}{\svmu}, \ssrsqopt = \qiform{\svsig}{\svmu}$.

Given \ssiz \iid observations of \vreti, let \amreti be the matrix
whose rows are the vectors \tr{\avreti[i]}. The na\"{i}ve sample estimator
\begin{equation}
\svsm \defeq \oneby{\ssiz}\gram{\amreti}
\end{equation}
is an unbiased estimator since $\pvsm = \E{\gram{\avreti}}$.


\subsection{Asymptotic distribution of the Markowitz portfolio}%FOLDUP

\label{subsec:dist_markoport}
\nocite{BrittenJones1999}

Collecting the mean and covariance into the second moment matrix 
gives the asymptotic distribution of the sample Markowitz
portfolio without much work. In some sense, this computation
generalizes the `standard' asymptotic analysis of Sharpe ratio of
multiple assets. \cite{jobsonkorkie1981,lo2002,mertens2002comments,Ledoit2008850,Leung2008,Wright2012} 

Let \fvec{\Mtx{A}}, and \fvech{\Mtx{A}} be the vector and half-space
vector operators.  The former turns an $\nlatf\times\nlatf$ matrix into
an $\nlatf^2$ vector of its columns stacked on top of each other; the 
latter vectorizes a symmetric (or lower triangular) matrix into a vector
of the non-redundant elements. \cite{magnus1980elimination} 

Define, as we have above, \svsm to be the unbiased sample estimate
of \pvsm, based on \ssiz \iid samples of \avreti.
Under the multivariate central limit theorem \cite{wasserman2004all}
\begin{equation}
\sqrt{\ssiz}\wrapParens{\fvech{\svsm} - \fvech{\pvsm}} 
\rightsquigarrow 
\normlaw{0,\pvvar},
\label{eqn:mvclt_svsm}
\end{equation}
where \pvvar is the variance of \fvech{\svsm}, which, in general, 
is unknown. For the case where \vreti is multivariate Gaussian,
\pvvar is known; see \subsecref{gaussian_fisher_inf}.

The Markowitz portfolio appears in $-\minv{\svsm}$. Let \Elim
be the `Elimination Matrix,' a matrix of zeros and ones with the
property that $\fvech{\Mtx{A}} = \Elim\fvec{\Mtx{A}}.$  
\cite{magnus1980elimination} We can find the asymptotic distribution
of $\minv{\svsm}$ via the delta method. The derivative of the matrix
inverse is given by
\begin{equation}
\dbyd{\fvec{\minv{\Mtx{A}}}}{\fvec{\Mtx{A}}} 
= - \AkronA{\minv{\Mtx{A}}},
\label{eqn:deriv_matrix_inverse}
\end{equation}
for symmetric \Mtx{A}.  \cite{magnus1980elimination,facklernotes}
We can reduce this to the non-redundant parts via the Elimination matrix:
\begin{equation}
\dbyd{\fvech{\minv{\Mtx{A}}}}{\fvech{\Mtx{A}}} 
= \qoform{\dbyd{\fvec{\minv{\Mtx{A}}}}{\fvec{\Mtx{A}}}}{\Elim}
= - \qoform{\wrapParens{\AkronA{\minv{\Mtx{A}}}}}{\Elim}.
\label{eqn:deriv_vech_matrix_inverse}
\end{equation}

Then we have, via the delta method, 
%\begin{equation}
\begin{multline}
\sqrt{\ssiz}\wrapParens{\fvech{\minv{\svsm}} - \fvech{\minv{\pvsm}}} 
\rightsquigarrow \\
\normlaw{0,\qform{\pvvar}{\wrapBracks{\qoform{\wrapParens{\AkronA{\minv{\pvsm}}}}{\Elim}}}}.
\label{eqn:mvclt_isvsm}
\end{multline}
%\end{equation}
To estimate the covariance of $\fvech{\minv{\svsm}} - \fvech{\minv{\pvsm}}$,
plug in \svsm for \pvsm in the covariance computation, and use some
consistent estimator for \pvvar, call 
it \svvar. 
%Rather, one must estimate $\qform{\pvvar}{\Elim}$.
The simple sample estimate can be had by computing the sample covariance
of the vectors $\fvech{\ogram{\avreti[i]}} =
\asvec{1,\tr{\vreti[i]},\tr{\fvech{\ogram{\vreti[i]}}}}$. 
More elaborate covariance estimators can be used, for example, to deal with
violations of the \iid assumptions. \cite{Zeileis:2004:JSSOBK:v11i10}

\nocite{magnus1999matrix,magnus1980elimination}
\nocite{BrittenJones1999}

Empirically, the marginal Wald test for zero weighting in the 
Markowitz portfolio based on this approximation are nearly
identical to the \tstat-statistics produced by the procedure
of Britten-Jones, as shown below. \cite{BrittenJones1999} 

<<'me_vs_bjones',echo=TRUE>>=
nday <- 1024
nstk <- 5

# under the null: all returns are zero mean;
set.seed(as.integer(charToRaw("7fbb2a84-aa4c-4977-8301-539e48355a35")))
rets <- matrix(rnorm(nday * nstk),nrow=nday)

# t-stat via Britten-Jones procedure
bjones.ts <- function(rets) {
	ones.vec <- matrix(1,nrow=dim(rets)[1],ncol=1)
	bjones.mod <- lm(ones.vec ~ rets - 1)
	bjones.sum <- summary(bjones.mod)
	retval <- bjones.sum$coefficients[,3]
}
# wald stat via inverse second moment trick
ism.ws <- function(rets) {
# flipping the sign on returns is idiomatic,
	asymv <- ism_vcov(- rets)
	asymv.mu <- asymv$mu[1:asymv$p]
	asymv.Sg <- asymv$Ohat[1:asymv$p,1:asymv$p]
	retval <- asymv.mu / sqrt(diag(asymv.Sg))
}

bjones.tstat <- bjones.ts(rets)
ism.wald <- ism.ws(rets)

# compare them:
print(bjones.tstat)
print(ism.wald)

# repeat under the alternative;
set.seed(as.integer(charToRaw("a5f17b28-436b-4d01-a883-85b3e5b7c218")))
zero.rets <- t(matrix(rnorm(nday * nstk),nrow=nday))
mu.vals <- (1/sqrt(253)) * seq(-1,1,length.out=nstk) 
rets <- t(zero.rets + mu.vals)

bjones.tstat <- bjones.ts(rets)
ism.wald <- ism.ws(rets)

# compare them:
print(bjones.tstat)
print(ism.wald)
@

%UNFOLD

\subsection{Unified Multivariate Gaussian}%FOLDUP

Note that
$$
\qiform{\pvsig}{\wrapParens{\vreti - \pvmu}} = 
\qiform{\pvsm}{\avreti} - 1.
$$
Using the block determinant formula, we find that \pvsm has the same 
determinant as \pvsig, that is $\det{\pvsm} = \det{\pvsig}.$  These
relationships hold without assuming a particular distribution for 
\vreti. 

Assume, now, that \vreti is multivariate Gaussian. 
Then the density of \vreti can be expressed more simply as
\begin{equation*}
\begin{split}
\normpdf{\vreti}{\pvmu,\pvsig} &= \frac{1}{\sqrt{\wrapParens{2\pi}^{\nlatf}\det{\pvsig}}} 
\longexp{-\half \qiform{\pvsig}{\wrapParens{\vreti - \pvmu}}},\\
 &= \frac{\wrapParens{\det{\pvsig}}^{-\half}}{\wrapParens{2\pi}^{\nlatf/2}}
\longexp{-\half \wrapParens{\qiform{\pvsm}{\avreti} - 1}},\\
 &= \wrapParens{2\pi}^{-\nlatf/2} \wrapParens{\det{\pvsm}}^{-\half}
\longexp{-\half \wrapParens{\qiform{\pvsm}{\avreti} - 1}},\\
 &= \wrapParens{2\pi}^{-\nlatf/2}
\longexp{\half - \half \logdet{\pvsm} - \half \trace{\minv{\pvsm}\ogram{\avreti}}},\\
\therefore - \log\normpdf{\vreti}{\pvmu,\pvsig} &= 
  c_{\nlatf} 
+ \half \logdet{\pvsm} 
+ \half \trace{\minv{\pvsm}\ogram{\avreti}},
\end{split}
\end{equation*}
for the constant 
$c_{\nlatf} = \exp{\half}-\half[\nlatf]\log\wrapParens{2\pi}.$

Given \ssiz \iid observations of \vreti, let \amreti be the matrix
whose rows are the vectors \tr{\vreti[i]}. Then the negative log
density of \amreti is
\begin{equation*}
- \log\normpdf{\amreti}{\pvsm} =
  \ssiz c_{\nlatf} 
+ \half[\ssiz] \logdet{\pvsm} 
+ \half \trace{\minv{\pvsm}\gram{\amreti}}.
\end{equation*}
Again let $\svsm = \gram{\amreti}/\ssiz$, the unbiased
sample estimate of \pvsm. Then 
\begin{equation*}
\frac{- 2\log\normpdf{\amreti}{\pvsm}}{\ssiz} = 
  c_{\nlatf} 
+ \logdet{\pvsm} 
+ \trace{\minv{\pvsm}\svsm}.
\end{equation*}

By Lemma (5.1.1) of Press \cite{press2012applied}, this can be expressed
as a density on \svsm, which is a sufficient statistic:
\begin{equation*}
\begin{split}
\frac{- 2\log\FOOpdf{}{\svsm}{\pvsm}}{\ssiz} 
&= \frac{- 2\log\normpdf{\amreti}{\pvsm}}{\ssiz}
	-\frac{2}{\ssiz}\wrapParens{\half[\ssiz-\nlatf-2]\logdet{\svsm}}\\
&\phantom{=}\,
	-\frac{2}{\ssiz}\wrapParens{\half[\nlatf+1]\wrapParens{\ssiz - \half[\nlatf]} \log\pi -
	\sum_{j=1}^{\nlatf+1} \log\funcit{\Gamma}{\half[\ssiz +1-j]}},\\
&= c_{\nlatf} - \frac{\nlatf+1}{\ssiz}\wrapParens{\ssiz - \half[\nlatf]} \log\pi 
	- \frac{2}{\ssiz} \sum_{j=1}^{\nlatf+1}
	\log\funcit{\Gamma}{\half[\ssiz +1-j]}\\
&\phantom{=}\,
	+ \logdet{\pvsm} - \frac{\ssiz-\nlatf-2}{\ssiz}\logdet{\svsm}
	+ \trace{\minv{\pvsm}\svsm},\\
&= c'_{\ssiz,\nlatf} 
	+ \logdet{\pvsm} - \frac{\ssiz-\nlatf-2}{\ssiz}\logdet{\svsm}
	+ \trace{\minv{\pvsm}\svsm},\\
&= c'_{\ssiz,\nlatf} 
	- \logdet{\minv{\pvsm}} - \frac{\ssiz-\nlatf-2}{\ssiz}\logdet{\svsm}
	+ \trace{\minv{\pvsm}\svsm}.
\end{split}
\end{equation*}

The density of \svsm is thus
\begin{equation}
\FOOpdf{}{\svsm}{\pvsm} =
c''_{\ssiz,\nlatf}\frac{\det{\svsm}^{\half[\ssiz-\nlatf-2]}}{\det{\pvsm}^{\half[\ssiz]}}
\longexp{-\half[\ssiz]\trace{\minv{\pvsm}\svsm}}.
\end{equation}
Thus $\ssiz\svsm$ has the same density, up to the leading constant, as a 
$\nlatf+1$-dimensional Wishart random variable with \ssiz degrees of freedom
and scale matrix \pvsm. In fact, $\ssiz\svsm$ is a \emph{conditional} Wishart,
conditional on $\svsm_{1,1} = 1$.
%conditional on $\svsm_{\nlatf+1,\nlatf+1} = 1$.  % for the other form.
%UNFOLD

\subsection{Maximum Likelihood Estimator}%FOLDUP

%The maximum likelihood estimator of \ichol{\pvsm} is found by 
%taking the derivative of the (log) likelihood and finding a root.
%That is,
%\begin{equation*}
%\begin{split}
%0 &= \drbydr{\log\FOOpdf{}{\svsm}{\pvsm}}{\ichol{\pvsm}},\\
  %&= -2 \trminv{\wrapParens{\ichol{\pvsm[m]}}} + \ichol{\pvsm[m]}2\svsm,
%\end{split}
%\end{equation*}
%leaning heavily on the Matrix Cookbook for 
%the derivatives. \cite{petersen2006matrix}
%This is solved by $\ichol{\pvsm[m]} = \ichol{\svsm}$, and thus the
%na\"{i}ve sample estimate is the MLE.
\providecommand{\txtMLE}{\mbox{MLE}}

The maximum likelihood estimator of \pvsm is found by 
taking the derivative of the (log) likelihood with respect to
\pvsm and finding a root. However, the derivative of log likelihood
with respect to \pvsm is mildly unpleasant:
\begin{equation}
\begin{split}
\drbydr{\log\FOOpdf{}{\svsm}{\pvsm}}{\pvsm} 
	&= - \half[\ssiz]\drbydr{\logdet{\pvsm}}{\pvsm} 
	- \half[\ssiz]\drbydr{\trace{\minv{\pvsm}\svsm}}{\pvsm},\\
	&= - \half[\ssiz]\minv{\pvsm}
	+ \half[\ssiz]\minv{\pvsm}\svsm\minv{\pvsm},
\end{split}
\label{eqn:mle_deadend}
\end{equation}
However, the derivative with respect to \minv{\pvsm} is a bit
simpler:
\begin{equation}
\begin{split}
\drbydr{\log\FOOpdf{}{\svsm}{\pvsm}}{\minv{\pvsm}} 
	&= \half[\ssiz]\drbydr{\logdet{\minv{\pvsm}}}{\minv{\pvsm}} 
	- \half[\ssiz]\drbydr{\trace{\minv{\pvsm}\svsm}}{\minv{\pvsm}},\\
  &= \half[\ssiz]\wrapParens{\pvsm - \svsm}.
\end{split}
\end{equation}
(See Magnus and Neudecker or the Matrix Cookbook for a refresher
on matrix derivatives. \cite{magnus1999matrix,petersen2006matrix})
Thus the likelihood is maximized by 
$\pvsm[\txtMLE] = \svsm$, \ie the unbiased sample estimator is also
the MLE. Note that this is also a root of \eqnref{mle_deadend}.

Since $\pvsm[\txtMLE] = \svsm$, the log likelihood of the MLE is 
\begin{equation}
\begin{split}
\log\FOOlik{}{\svsm}{\pvsm[\txtMLE]} 
 &= - \half[\ssiz] c'_{\ssiz,\nlatf} - \half[\ssiz] \logdet{\pvsm[\txtMLE]} +
 \half[\ssiz-\nlatf-2]\logdet{\svsm}\\
&\phantom{=}\,+ \trace{\minv{\pvsm[\txtMLE]}\svsm},\\
 &= -\half[\ssiz] c'_{\ssiz,\nlatf} 
 - \half[\nlatf+2]\logdet{\svsm} + \wrapParens{\nlatf+1}.
\end{split}
\end{equation}

\subsubsection{Fisher Information}

\label{subsec:gaussian_fisher_inf}

\providecommand{\FishI}[1][{}]{\mathSUB{\mathcal{I}}{#1}}
\providecommand{\FishIf}[2][{}]{\funcit{\mathSUB{\mathcal{I}}{#1}}{#2}}
\providecommand{\qfElim}[1]{\qform{#1}{\Elim}}
\providecommand{\qoElim}[1]{\qoform{#1}{\Elim}}
\providecommand{\qoElim}[1]{\qoform{#1}{\Elim}}

To compute the Fisher Information, first compute
the Hessian of $\log\FOOpdf{}{\svsm}{\pvsm}$ with
respect to $\fvech{\minv{\pvsm}}$. From above we know that
\begin{equation*}
\drbydr{\log\FOOpdf{}{\svsm}{\pvsm}}{\minv{\pvsm}}
 = \half[\ssiz]\wrapParens{\pvsm - \svsm}.
\end{equation*}
So
\begin{equation*}
\begin{split}
\drbydr[2]{\log\FOOpdf{}{\svsm}{\pvsm}}{\minv{\pvsm}}
 & \defeq
\drbydr{\fvech{\drbydr{\log\FOOpdf{}{\svsm}{\pvsm}}{\minv{\pvsm}}}}{\fvech{\minv{\pvsm}}},\\
 &= - \half[\ssiz] \qoElim{\wrapParens{\AkronA{\pvsm}}},
\end{split}
\end{equation*}
via \eqnref{deriv_vech_matrix_inverse}, where, again, \Elim is the 
`Elimination Matrix.' \cite{magnus1980elimination}

Thus 
\begin{equation}
\FishIf[\ssiz]{\minv{\pvsm}} = \half[\ssiz]
\qoElim{\wrapParens{\AkronA{\pvsm}}}.
\end{equation}
By change of variables, and again using \eqnref{deriv_vech_matrix_inverse}, 
the Fisher Information with respect to \pvsm is
\begin{equation}
\FishIf[\ssiz]{\pvsm} = \half[\ssiz]
\qform{\wrapParens{\qoElim{\wrapParens{\AkronA{\pvsm}}}}}{\wrapBracks{\qoElim{\wrapParens{\AkronA{\minv{\pvsm}}}}}}.
\end{equation}

Under `the appropriate regularity conditions,' \cite{wasserman2004all}
\begin{equation}
\label{eqn:converge_theta}
\begin{split}
\wrapParens{\fvech{\minv{\svsm}} - \fvech{\minv{\pvsm}}} 
& \rightsquigarrow 
\normlaw{0,\minv{\wrapParens{\FishIf[\ssiz]{\minv{\pvsm}}}}},\\
\wrapParens{\fvech{\svsm} - \fvech{\pvsm}} 
& \rightsquigarrow 
\normlaw{0,\minv{\wrapParens{\FishIf[\ssiz]{{\pvsm}}}}}.
\end{split}
\end{equation}
Thus for the case of multivariate Gaussian returns, the term \pvvar
from \eqnref{mvclt_svsm} is identified as
\begin{equation}
\begin{split}
\label{eqn:pvvar_identity}
\pvvar &= \ssiz\minv{\wrapParens{\FishIf[\ssiz]{{\pvsm}}}},\\
 &= 2
\minv{\wrapParens{\qform{\wrapParens{\qoElim{\wrapParens{\AkronA{\pvsm}}}}}{\wrapBracks{\qoElim{\wrapParens{\AkronA{\minv{\pvsm}}}}}}}}.
\end{split}
\end{equation}

%UNFOLD

\subsection{Likelihood Ratio Test}%FOLDUP

Suppose that $\pvsm[0]$ is the maximum likelihood estimate of \pvsm under
some null hypothesis under consideration. The likelihood ratio test
statistic is 
\begin{equation}
\begin{split}
-2\log\Lambda &\defeq
-2\log\wrapParens{\frac{\FOOlik{}{\svsm}{\pvsm[0]}}{\FOOlik{}{\svsm}{\pvsm[\txtMLE]}}},\\
&= \ssiz\wrapParens{\logdet{\pvsm[0]\minv{\pvsm[\txtMLE]}} + 
\trace{\wrapBracks{\minv{\pvsm[0]} - \minv{\pvsm[\txtMLE]}}\svsm}},\\
&= \ssiz\wrapParens{\logdet{\pvsm[0]\minv{\svsm}} + 
\trace{\minv{\pvsm[0]}\svsm} - \wrapBracks{\nlatf + 1}}.
\end{split}
\label{eqn:wilks_lambda_def}
\end{equation}

\providecommand{\lrtA}[1][i]{\mathSUB{\Mtx{A}}{#1}}
\providecommand{\lrta}[1][i]{\mathSUB{a}{#1}}


\subsubsection{Tests on the Precision and Markowitz Portfolio}%FOLDUP

%This section is based on Dempster's ``Covariance Selection''. \cite{dempster1972}


For some conformable symmetric matrices \lrtA, and given scalars \lrta, 
consider the null hypothesis
\begin{equation}
H_0: \trace{\lrtA[i]\minv{\pvsm}} = \lrta[i],\,i=1,\ldots,m.
\label{eqn:lrt_null_back}
\end{equation}
The constraints have to be sensible. 
For example, they cannot
violate the positive definiteness of 
\minv{\pvsm}, \etc Without loss of generality, we can assume
that the \lrtA[i] are symmetric, since \pvsm is symmetric, and for
symmetric \Mtx{G} and square \Mtx{H}, 
$\trace{\Mtx{G}\Mtx{H}} = \trace{\Mtx{G}\half\wrapParens{\Mtx{H} +
\tr{\Mtx{H}}}}$, and so we could replace any non-symmetric \lrtA[i] with
$\half\wrapParens{\lrtA[i] + \tr{\lrtA[i]}}$.

Employing the Lagrange multiplier technique, the maximum likelihood
estimator under the null hypothesis satisfies
\begin{equation*}
\begin{split}
0 &= \drbydr{\log\FOOpdf{}{\svsm}{\pvsm}}{\minv{\pvsm}}
- \sum_i \lambda_i
%\drbydr{\trace{\lrtA[i]\minv{\pvsm}}}{\ichol{\pvsm}},\\
\drbydr{\trace{\lrtA[i]\minv{\pvsm}}}{\minv{\pvsm}},\\
&= - \pvsm + \svsm - \sum_i \lambda_i \lrtA[i],\\
\therefore \pvsm[\txtMLE] &= \svsm - \sum_i \lambda_i \lrtA[i].
\end{split}
\end{equation*}
The maximum likelihood estimator under the constraints has to be
found numerically by solving for the $\lambda_i$, subject
to the constraints in \eqnref{lrt_null_back}.

This framework slightly generalizes Dempster's 
``Covariance Selection.'' \cite{dempster1972} Covariance selection
reduces to the case where each \lrta[i] is zero, and
each \lrtA[i] is a matrix of all zeros except two (symmetric) ones 
somewhere in the lower right $\nlatf \times \nlatf$ sub matrix. In
all other respects, however, the solution here follows Dempster.

\providecommand{\vitrlam}[2]{\vectUL{\lambda}{\wrapNeParens{#1}}{#2}}
\providecommand{\sitrlam}[2]{\mathUL{\lambda}{\wrapNeParens{#1}}{#2}}
\providecommand{\vitrerr}[2]{\vectUL{\epsilon}{\wrapNeParens{#1}}{#2}}
\providecommand{\sitrerr}[2]{\mathUL{\epsilon}{\wrapNeParens{#1}}{#2}}

An iterative technique for finding the MLE based on a Newton step 
would proceed as follow.  \cite{nocedal2006numerical} 
Let \vitrlam{0}{} be some initial estimate of the
vector of $\lambda_i$. (A good initial estimate can likely be had 
by abusing the asymptotic normality 
result from \subsecref{dist_markoport}.)
The residual of the \kth{k} estimate, \vitrlam{k}{} is
\begin{equation}
\vitrerr{k}{i} \defeq 
\trace{\lrtA[i]\minv{\wrapBracks{\svsm - \sum_j \sitrlam{k}{j} \lrtA[j]}}} - \lrta[i].
\end{equation}
The Jacobian of this residual with respect to the \kth{l} element of \vitrlam{k}
is
\begin{equation}
\begin{split}
\drbydr{\vitrerr{k}{i}}{\sitrlam{k}{l}} &= 
\trace{\lrtA[i]\minv{\wrapBracks{\svsm - \sum_j \sitrlam{k}{j} \lrtA[j]}}
\lrtA[l] \minv{\wrapBracks{\svsm - \sum_j \sitrlam{k}{j} \lrtA[j]}}},\\
&= \tr{\fvec{\lrtA[i]}} \wrapParens{\AkronA{\minv{\wrapBracks{\svsm - \sum_j
\sitrlam{k}{j}\lrtA[j]}}}} \fvec{\lrtA[l]}.
\end{split}
\end{equation}

Newton's method is then the iterative scheme
\begin{equation}
\vitrlam{k+1}{} \leftarrow \vitrlam{k}{} -
\minv{\wrapParens{\drbydr{\vitrerr{k}{}}{\vitrlam{k}{}}}} \vitrerr{k}.
\end{equation}
When (if?) the iterative scheme converges on the optimum, one can 
compute the likelihood ratio statistic $-2\log\Lambda$, as defined
in \eqnref{wilks_lambda_def}. By Wilks' Theorem, under the null 
hypothesis, $-2\log\Lambda$ is, asymptotically in \ssiz, distributed as 
a chi-square with $m$ degrees of freedom. \cite{wilkstheorem1938}
%UNFOLD

%\subsubsection{Tests on the Mean and Covariance Matrix}%FOLDUP

%For some conformable symmetric matrices \lrtA, and given scalars \lrta, 
%consider the null hypothesis
%\begin{equation}
%H_0: \trace{\lrtA[i]{\pvsm}} = \lrta[i],\,i=1,\ldots,m.
%\label{eqn:lrt_null_forw}
%\end{equation}
%The constraints have to be sensible. For example, they cannot
%violate the positive definiteness of 
%\pvsm, \etc  Without loss of generality, we can assume
%that the \lrtA[i] are symmetric, since \pvsm is symmetric, and for
%symmetric \Mtx{G} and square \Mtx{H}, 
%$\trace{\Mtx{G}\Mtx{H}} = \trace{\Mtx{G}\half\wrapParens{\Mtx{H} +
%\tr{\Mtx{H}}}}$, and so we could replace any non-symmetric \lrtA[i] with
%$\half\wrapParens{\lrtA[i] + \tr{\lrtA[i]}}$.

%Employing the Lagrange multiplier technique, the maximum likelihood
%estimator under the null hypothesis satisfies
%\begin{equation*}
%\begin{split}
%0 &= \drbydr{\log\FOOpdf{}{\svsm}{\pvsm}}{\minv{\pvsm}}
%- \sum_i \lambda_i
%%\drbydr{\trace{\lrtA[i]\minv{\pvsm}}}{\ichol{\pvsm}},\\
%\drbydr{\trace{\lrtA[i]{\pvsm}}}{\minv{\pvsm}},\\
%&= - \pvsm + \svsm + \sum_i \lambda_i \pvsm \lrtA[i] \pvsm,\\
%\therefore \pvsm[\txtMLE] &= ???.
%\end{split}
%\end{equation*}
%The maximum likelihood estimator under the constraints has to be
%found numerically by solving for the $\lambda_i$, subject
%to the constraints in \eqnref{lrt_null_back}.


%%UNFOLD
%UNFOLD

% 2FIX: add examples here ... 
%UNFOLD

% bibliography%FOLDUP
\nocite{markowitz1952portfolio,markowitz1999early,markowitz2012foundations}
%\bibliographystyle{jss}
%\bibliographystyle{siam}
%\bibliographystyle{ieeetr}
\bibliographystyle{plainnat}
%\bibliographystyle{acm}
\bibliography{SharpeR,rauto}
%UNFOLD

\end{document}
%for vim modeline: (do not edit)
% vim:fdm=marker:fmr=FOLDUP,UNFOLD:cms=%%s:syn=rnoweb:ft=rnoweb:nu
