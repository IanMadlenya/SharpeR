%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Asymptotic Distribution of the Markowitz Portfolio}
%\VignetteKeyword{Finance}
%\VignetteKeyword{Sharpe}
%\VignettePackage{SharpeR}
\documentclass[10pt,a4paper,english]{article}

% front matter%FOLDUP
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}
% for therefore
\usepackage{amssymb}
% for theorems?
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\usepackage{hyperref}
\usepackage[square,numbers]{natbib}
%\usepackage[authoryear]{natbib}
%\usepackage[iso]{datetime}
%\usepackage{datetime}

%compactitem and such:
\usepackage[newitem,newenum,increaseonly]{paralist}

\makeatletter
\makeatother

%\input{sr_defs.tex}
\usepackage{SharpeR}

\providecommand{\sideWarning}[1][0.5]{\marginpar{\hfill\includegraphics[width=#1\marginparwidth]{warning}}}

% knitr setup%FOLDUP

<<'preamble', include=FALSE, warning=FALSE, message=FALSE>>=
library(knitr)

# set the knitr options ... for everyone!
# if you unset this, then vignette build bonks. oh, joy.
#opts_knit$set(progress=TRUE)
opts_knit$set(eval.after='fig.cap')
# for a package vignette, you do want to echo.
# opts_chunk$set(echo=FALSE,warning=FALSE,message=FALSE)
opts_chunk$set(warning=FALSE,message=FALSE)
#opts_chunk$set(results="asis")
opts_chunk$set(cache=TRUE,cache.path="cache/SharpeRatio")

#opts_chunk$set(fig.path="figure/",dev=c("pdf","cairo_ps"))
opts_chunk$set(fig.path="figure/SharpeRatio",dev=c("pdf"))
opts_chunk$set(fig.width=5,fig.height=4,dpi=64)

# doing this means that png files are made of figures;
# the savings is small, and it looks like shit:
#opts_chunk$set(fig.path="figure/",dev=c("png","pdf","cairo_ps"))
#opts_chunk$set(fig.width=4,fig.height=4)
# for figures? this is sweave-specific?
#opts_knit$set(eps=TRUE)

# this would be for figures:
#opts_chunk$set(out.width='.8\\textwidth')
# for text wrapping:
options(width=64,digits=2)
opts_chunk$set(size="small")
opts_chunk$set(tidy=TRUE,tidy.opts=list(width.cutoff=50,keep.blank.line=TRUE))

compile.time <- Sys.time()

# from the environment

# only recompute if FORCE_RECOMPUTE=True w/out case match.
FORCE_RECOMPUTE <- 
	(toupper(Sys.getenv('FORCE_RECOMPUTE',unset='False')) == "TRUE")

# compiler flags!

# not used yet
LONG.FORM <- FALSE

mc.resolution <- ifelse(LONG.FORM,1000,200)
mc.resolution <- max(mc.resolution,100)

library(quantmod)
options("getSymbols.warning4.0"=FALSE)

library(SharpeR)

gen_norm <- rnorm
lseq <- function(from,to,length.out) { 
	exp(seq(log(from),log(to),length.out = length.out))
}
@
%UNFOLD
%UNFOLD

% document incantations%FOLDUP
\begin{document}

\title{Asymptotic Distribution of the \txtMwtz Portfolio}
\author{Steven E. Pav \thanks{\email{spav@alumni.cmu.edu}}}
%\date{\today, \currenttime}

\maketitle
%UNFOLD

\begin{abstract}%FOLDUP
The asymptotic distribution of the \txtMP, \minvAB{\svsig}{\svmu},
is derived, for the general case (assuming fourth moments of returns exist), 
and for the case of multivariate normal returns. 
The derivation allows for inference which is robust to heteroskedasticity 
and autocorrelation of moments up to order four. As a side effect, one 
can estimate the proportion of error in the \txtMP due to 
mis-estimation of the covariance matrix. A likelihood ratio test is given
which generalizes Dempster's Covariance Selection test to allow inference
on linear combinations of the precision matrix and the 
\txtMP. \cite{dempster1972}
\end{abstract}%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}%FOLDUP

Given \nlatf assets with expected return \pvmu and covariance of return \pvsig,
the portfolio defined as 
\begin{equation}
\pportwopt \defeq \lambda \minvAB{\pvsig}{\pvmu}
\end{equation}
plays a special role in modern portfolio 
theory. \cite{markowitz1952portfolio,brandt2009portfolio,GVK322224764}
It is known as the `efficient portfolio', the `tangency portfolio', 
and, somewhat informally, the `\txtMP'. 
It appears, for various $\lambda$, in the solution to numerous
portfolio optimization problems.  
Besides the classic mean-variance formulation,
it solves the (population) \txtSR maximization problem:
\begin{equation}
\max_{\pportw : \qform{\pvsig}{\pportw} \le \Rbuj^2} 
\frac{\trAB{\pportw}{\pvmu} - \rfr}{\sqrt{\qform{\pvsig}{\pportw}}},
\label{eqn:opt_port_I}
\end{equation}
where $\rfr\ge 0$ is the risk-free, or `disastrous', rate of return, and 
$\Rbuj > 0$ is some given `risk budget'. 
The solution to this optimization problem is $\lambda \minvAB{\pvsig}{\pvmu}$,
where $\lambda = \fracc{\Rbuj}{\sqrt{\qiform{\pvsig}{\pvmu}}}.$
\nocite{markowitz1952portfolio}  % does not actually mention this portfolio?

In practice, the \txtMP has a somewhat checkered history. 
The population parameters \pvmu and \pvsig are not known and must
be estimated from samples. Estimation error results in a feasible
portfolio, \sportwopt, of dubious value. Michaud went so far as to call
mean-variance optimization, 
``error maximization.'' \cite{michaud1989markowitz}  
It has been suggested that simple portfolio heuristics outperform the
\txtMP in practice.  \cite{demiguel2009optimal}  

This paper focuses on the asymptotic distribution of the sample \txtMP. 
By formulating the problem as a linear regression, Britten-Jones 
very cleverly devised hypothesis tests on elements of \pportwopt,
assuming multivariate Gaussian returns. \cite{BrittenJones1999}  
In a remarkable series of papers, Okhrin and Schmid, and 
Bodnar and Okhrin give the (univariate) density of
the dot product of \pportwopt and a deterministic vector, again for the
case of Gaussian returns. \cite{okhrin2006distributional,SJOS:SJOS729}  
Okhrin and Schmid also show that all moments of
$\fracc{\sportwopt}{\trAB{\vone}{\sportwopt}}$ of order greater than or
equal to one do not exist. \cite{okhrin2006distributional}

Here I derive asymptotic normality of \sportwopt, the sample
analogue of \pportwopt, assuming only that the first four moments
exist. Feasible estimation of the variance of \sportwopt is amenable
to heteroskedasticity and autocorrelation robust 
inference. \cite{Zeileis:2004:JSSOBK:v11i10}
The asymptotic distribution under Gaussian returns is also derived.

After estimating the covariance of \sportwopt, one can compute Wald
test statistics for the elements of \sportwopt, possibly leading one
to drop some assets from consideration (`sparsification'). Having
an estimate of the covariance can also allow portfolio shrinkage.
\cite{demiguel2013size,kinkawa2010estimation}

The derivations in this paper actually solve a more general problem
than the distribution of the sample \txtMP. The covariance of 
\sportwopt and the `precision matrix,' \minv{\svsig} are derived.
This allows one, for example, to estimate the proportion of error
in the \txtMP attributable to mis-estimation of the covariance
matrix. According to lore, the error in portfolio weights is
mostly attributable to mis-estimation of \pvmu, 
not of \pvsig. \cite{chopra1993effect,NBERw0444}

Finally, assuming Gaussian returns, a likelihood ratio test for
performing inference on linear combinations of elements of
the \txtMP and the precision matrix is derived. This test
generalizes a procedure by Dempster for inference on
the precision matrix alone. \cite{dempster1972}

% \cite{tu2011markowitz}  % is this needed? really. talmud ... 
%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The augmented second moment}%FOLDUP

Let \vreti be an array of returns of \nlatf assets, with mean \pvmu, and
covariance \pvsig.  Let \avreti be \vreti prepended with a 1:
$\avreti = \asvec{1,\tr{\vreti}}$. Consider the second
moment of \avreti:
\begin{equation}
\pvsm \defeq \E{\ogram{\avreti}} = 
	\twobytwo{1}{\tr{\pvmu}}{\pvmu}{\pvsig + \ogram{\pvmu}}.
\label{eqn:pvsm_def}
\end{equation}
By inspection one can confirm that the
inverse of \pvsm is
\begin{equation}
\minv{\pvsm} 
= \twobytwo{1 + \qiform{\pvsig}{\pvmu}}{-\tr{\pvmu}\minv{\pvsig}}{-\minv{\pvsig}\pvmu}{\minv{\pvsig}}
= \twobytwo{1 + \psnrsqopt}{-\tr{\pportwopt}}{-\pportwopt}{\minv{\pvsig}},
\label{eqn:trick_inversion}
\end{equation}
where $\pportwopt=\minvAB{\pvsig}{\svmu}$ is the \txtMP,
and $\psnropt=\sqrt{\qiform{\pvsig}{\pvmu}}$ is the \txtSR of
that portfolio. The matrix \pvsm contains the first and second
moment of \vreti, but is also the uncentered second moment of
\avreti, a fact which makes it amenable to analysis via the
central limit theorem.

The relationships above are merely facts of linear algebra, and so
hold for sample estimates as well:
\begin{equation*}
\minv{\twobytwo{1}{\tr{\svmu}}{\svmu}{\svsig + \ogram{\svmu}}}
= {\twobytwo{1 +
\ssrsqopt}{-\tr{\sportwopt}}{-\sportwopt}{\minv{\svsig}}},
\end{equation*}
where \svmu, \svsig are some sample estimates of \pvmu and \pvsig, and 
$\sportwopt = \minvAB{\svsig}{\svmu}, \ssrsqopt = \qiform{\svsig}{\svmu}$.

Given \ssiz \iid observations \vreti[i], let \amreti be the matrix
whose rows are the vectors \tr{\avreti[i]}. The na\"{i}ve sample estimator
\begin{equation}
\svsm \defeq \oneby{\ssiz}\gram{\amreti}
\end{equation}
is an unbiased estimator since $\pvsm = \E{\gram{\avreti}}$.

\subsection{Asymptotic distribution of the \txtMP}%FOLDUP

\label{subsec:dist_markoport}
\nocite{BrittenJones1999}

Collecting the mean and covariance into the second moment matrix 
gives the asymptotic distribution of the sample \txtMP
without much work. In some sense, this computation
generalizes the `standard' asymptotic analysis of Sharpe ratio of
multiple assets. \cite{jobsonkorkie1981,lo2002,Ledoit2008850,Leung2008} 
%\cite{jobsonkorkie1981,lo2002,mertens2002comments,Ledoit2008850,Leung2008,Wright2012} 

Let \fvec{\Mtx{A}}, and \fvech{\Mtx{A}} be the vector and half-space
vector operators.  The former turns an $\nlatf\times\nlatf$ matrix into
an $\nlatf^2$ vector of its columns stacked on top of each other; the 
latter vectorizes a symmetric (or lower triangular) matrix into a vector
of the non-redundant elements.  
Let \Elim be the `Elimination Matrix,' a matrix of zeros and ones with the
property that 
$\fvech{\Mtx{A}} = \Elim\fvec{\Mtx{A}}.$  \cite{magnus1980elimination} 

A technical lemma is needed.
\begin{lemma}%FOLDUP
For symmetric, invertible matrix \Mtx{A}, 
\begin{equation}
\dbyd{\fvech{\minv{\Mtx{A}}}}{\fvech{\Mtx{A}}} 
= - \qoform{\wrapParens{\AkronA{\minv{\Mtx{A}}}}}{\Elim}.
\label{eqn:deriv_vech_matrix_inverse}
\end{equation}
\label{lemma:deriv_vech_matrix_inverse}
\end{lemma}%UNFOLD
\begin{proof}%FOLDUP
The following is a standard result of 
matrix calculus, \cite{magnus1999matrix,petersen2012matrix,facklernotes}
\begin{equation}
\dbyd{\fvec{\minv{\Mtx{A}}}}{\fvec{\Mtx{A}}} 
= - \AkronA{\minv{\Mtx{A}}}.
\label{eqn:deriv_matrix_inverse}
\end{equation}
The non-redundant parts of this can be expressed via the
Elimination matrix,
\begin{equation*}
\dbyd{\fvech{\minv{\Mtx{A}}}}{\fvech{\Mtx{A}}} 
= \qoform{\dbyd{\fvec{\minv{\Mtx{A}}}}{\fvec{\Mtx{A}}}}{\Elim}
= - \qoform{\wrapParens{\AkronA{\minv{\Mtx{A}}}}}{\Elim}.
\end{equation*}
\end{proof}%UNFOLD

Note how \lemmaref{deriv_vech_matrix_inverse} generalizes the scalar case:
$$
\dbyd{x^{-1}}{x} = - \wrapParens{x^{-1} x^{-1}}.
$$
The asymptotic distribution of \minv{\pvsm} then follows from standard
techniques.
\begin{theorem}%FOLDUP
Let \svsm be the unbiased sample estimate of 
\pvsm, based on \ssiz \iid samples of \vreti.
Then, asymptotically in \ssiz, 
%\begin{equation}
\begin{multline}
\sqrt{\ssiz}\wrapParens{\fvech{\minv{\svsm}} - \fvech{\minv{\pvsm}}} 
\rightsquigarrow \\
\normlaw{0,\qform{\pvvar}{\wrapBracks{\qoform{\wrapParens{\AkronA{\minv{\pvsm}}}}{\Elim}}}},
\label{eqn:mvclt_isvsm}
\end{multline}
%\end{equation}
where \pvvar is the variance of \ogram{\avreti}.
Furthermore, we may replace \pvvar in this equation with an asymptotically
consistent estimator, \svvar.
\label{theorem:inv_distribution}
\end{theorem}%UNFOLD
\begin{proof}%FOLDUP
Under the multivariate central limit theorem \cite{wasserman2004all}
\begin{equation}
\sqrt{\ssiz}\wrapParens{\fvech{\svsm} - \fvech{\pvsm}} 
\rightsquigarrow 
\normlaw{0,\pvvar},
\label{eqn:mvclt_svsm}
\end{equation}
where \pvvar is the variance of \ogram{\avreti}, which, in general, 
is unknown. 
%(For the case where \vreti is multivariate Gaussian,
%\pvvar is known; see \theoremref{inv_distribution_gaussian}.)

By the delta method,
\begin{equation*}
\sqrt{\ssiz}\wrapParens{\fvech{\minv{\svsm}} - \fvech{\minv{\pvsm}}} 
\rightsquigarrow 
\normlaw{0,\qform{\pvvar}{\wrapBracks{\dbyd{\fvech{\minv{\pvsm}}}{\fvech{\pvsm}}}}}.
\end{equation*}
The derivative is given by \lemmaref{deriv_vech_matrix_inverse}, and
the result follows.
\end{proof}%UNFOLD

%To estimate the covariance of $\fvech{\minv{\svsm}} - \fvech{\minv{\pvsm}}$,
To estimate the covariance of $\fvech{\minv{\svsm}}$,
plug in \svsm for \pvsm in the covariance computation, and use some
consistent estimator for \pvvar, call 
it \svvar. 
%Rather, one must estimate $\qform{\pvvar}{\Elim}$.
One way to compute \svvar is to via the sample covariance of the
vectors $\fvech{\ogram{\avreti[i]}} =
\asvec{1,\tr{\vreti[i]},\tr{\fvech{\ogram{\vreti[i]}}}}$. 
More elaborate covariance estimators can be used, for example, to deal with
violations of the \iid assumptions. \cite{Zeileis:2004:JSSOBK:v11i10}
\nocite{magnus1999matrix,magnus1980elimination}
\nocite{BrittenJones1999}

%UNFOLD

\subsection{The \txtSR optimal portfolio}%FOLDUP

\begin{lemma}[\txtSR optimal portfolio]%FOLDUP
Assuming $\pvmu \ne \vzero$, and \pvsig is invertible,
the portfolio optimization problem
\begin{equation}
\argmax_{\pportw :\, \qform{\pvsig}{\pportw} \le \Rbuj^2} 
\frac{\trAB{\pportw}{\pvmu} - \rfr}{\sqrt{\qform{\pvsig}{\pportw}}},
\label{eqn:sr_optimal_portfolio_problem}
\end{equation}
for $\rfr \ge 0, \Rbuj > 0$ is solved by
\begin{equation}
\pportwoptR \defeq \frac{\Rbuj}{\sqrt{\qiform{\pvsig}{\pvmu}}}
\minvAB{\pvsig}{\pvmu}.
\end{equation}
Moreover, this is the unique solution whenever $\rfr > 0$.
The maximal objective achieved by this portfolio is
$\sqrt{\qiform{\pvsig}{\pvmu}} - \fracc{\rfr}{\Rbuj}$.
\label{lemma:sr_optimal_portfolio}
\end{lemma}%UNFOLD
\begin{proof}%FOLDUP
By the Lagrange multiplier technique, the optimal portfolio
solves the following equations:
\begin{equation*}
\begin{split}
0 &= c_1 \pvmu - c_2 \pvsig \pportw - \gamma \pvsig \pportw,\\
\qform{\pvsig}{\pportw} &\le \Rbuj^2,
\end{split}
\end{equation*}
where $\gamma$ is the Lagrange multiplier, and $c_1, c_2$ are 
scalar constants.
Solving the first equation gives us
$$
\pportw = c\,\minvAB{\pvsig}{\pvmu}.
$$
This reduces the problem to the univariate optimization
\begin{equation}
\max_{c :\, c^2 \le \fracc{\Rbuj^2}{\psnrsqopt}} 
\sign{c} \psnropt - \frac{\rfr}{\abs{c}\psnropt},
\end{equation}
where $\psnrsqopt = \qiform{\pvsig}{\pvmu}.$ The optimum
occurs for $c = \fracc{\Rbuj}{\psnropt}$, moreover the optimum
is unique when $\rfr > 0$.
\end{proof}%UNFOLD

Note that the first element of \fvech{\minv{\pvsm}} is $1 +
\qiform{\pvsig}{\pvmu}$, and elements 2 through $\nlatf+1$ are 
$-\pportwopt$. Thus, \pportwoptR, the portfolio that maximizes the \txtSR, 
is some transformation of \fvech{\minv{\pvsm}}, and another 
application of the delta method gives its asymptotic distribution,
as in the following corollary to \theoremref{inv_distribution}.

\begin{corollary}%FOLDUP
Let 
\begin{equation}
\pportwoptR = \frac{\Rbuj}{\sqrt{\qiform{\pvsig}{\pvmu}}}
\minvAB{\pvsig}{\pvmu},
\end{equation}
and similarly, let \sportwoptR be the sample analogue, where \Rbuj is
some risk budget. Then
\begin{multline}
\sqrt{\ssiz}\wrapParens{\sportwoptR - \pportwoptR} 
\rightsquigarrow \\
\normlaw{0,\qqform{\pvvar}{\wrapBracks{\qoform{\wrapParens{\AkronA{\minv{\pvsm}}}}{\Elim}}}{\Mtx{K}}},
\label{eqn:mvclt_portfolio}
\end{multline}
where 
$$
\Mtx{K} = - \Rbuj \asvec{\half \frac{\pportwoptR}{\psnrsqopt},
\oneby{\psnropt}\eye[\nlatf],\mzero},
$$
and $\psnrsqopt \defeq \qiform{\pvsig}{\pvmu}.$
\end{corollary}%UNFOLD
\begin{proof}%FOLDUP
By the delta method, and \theoremref{inv_distribution}, it suffices
to show that 
$$\Mtx{K} = \dbyd{\pportwoptR}{\fvech{\minv{\pvsm}}}.$$
To show this, note that \pportwoptR is $-\Rbuj$ times elements
2 through $\nlatf+1$ of \fvech{\minv{\pvsm}} divided by 
$\psnropt = \sqrt{\trAB{\basev[1]}{\fvech{\minv{\pvsm}}} - 1}$, where
$\basev[i]$ is the \kth{i} column of the identity matrix. The result
follows from basic calculus.
\end{proof}%UNFOLD

%% constrained case ... %FOLDUP
%% 2FIX: nstrat nstrathej fuck. as in, does this make sense in SharpeRatio.rnw? 
%% these show up as k and p, and what are you doing?

%Consider, now, the \emph{constrained} portfolio optimization problem,
%\begin{equation*}
%\max_{\pportw : \hejG\pvsig \pportw = \vzero,\, \qform{\pvsig}{\pportw} \le
%\Rbuj^2} 
%\frac{\trAB{\pportw}{\pvmu} - \rfr}{\sqrt{\qform{\pvsig}{\pportw}}},
%\end{equation*}
%where $\hejG$ is now a $\nlatfhej \times \nlatf$ matrix of 
%rank \nlatfhej.
%We can interpret the \hejG constraint as stating that the covariance of 
%the returns of a feasible portfolio with the returns of a portfolio 
%whose weights are in a given row of \hejG shall equal zero.
%In the garden variety application of this problem, \hejG consists of 
%\nlatfhej rows of the identity matrix;
%in this case, feasible portfolios are `hedged' with respect 
%to the \nlatfhej assets selected by \hejG
%(although they may hold some position in the hedged assets).

%\begin{lemma}[constrained \txtSR optimal portfolio]%FOLDUP
%Assuming $\pvmu \ne \vzero$, and \pvsig is invertible,
%the portfolio optimization problem
%\begin{equation}
%\max_{\pportw : \hejG\pvsig \pportw = \vzero,\, \qform{\pvsig}{\pportw} \le
%\Rbuj^2} 
%\frac{\trAB{\pportw}{\pvmu} - \rfr}{\sqrt{\qform{\pvsig}{\pportw}}},
%\label{eqn:cons_port_prob}
%\end{equation}
%for $\rfr \ge 0, \Rbuj > 0$ is solved by
%% (2FIX nomenclature)
%\begin{equation*}
%\begin{split}
%\pportwoptFoo{\Rbuj,\hejG,} &\defeq c \wrapParens{\minv{\pvsig}{\pvmu} -
	%\qiform{\wrapParens{\qoform{\pvsig}{\hejG}}}{\hejG}\pvmu},\\
 %c &= \frac{\Rbuj}{\sqrt{\qiform{\pvsig}{\pvmu} - 
	%\qiform{\wrapParens{\qoform{\pvsig}{\hejG}}}{\wrapParens{\hejG\pvmu}}}}.
%\end{split}
%\end{equation*}
%When $\rfr > 0$ the solution is unique.
%\label{lemma:cons_sr_optimal_portfolio}
%\end{lemma}%UNFOLD
%\begin{proof}%FOLDUP
%By the Lagrange multiplier technique, the optimal portfolio
%solves the following equations:
%\begin{equation*}
%\begin{split}
%0 &= c_1 \pvmu - c_2 \pvsig \pportw - \gamma_1 \pvsig \pportw -
%\pvsig\trAB{\hejG}{\vect{\gamma_2}},\\
%\qform{\pvsig}{\pportw} &\le \Rbuj^2,\\
%\hejG\pvsig \pportw &= \vzero,
%\end{split}
%\end{equation*}
%where $\gamma_i$ are Lagrange multipliers, and $c_1, c_2$ are 
%scalar constants.

%Solving the first equation gives
%$$
%\pportw = c_3\wrapBracks{\minvAB{\pvsig}{\pvmu} -
%\trAB{\hejG}{\vect{\gamma_2}}}.
%$$
%Reconciling this with the hedging equation we have
%$$
%\vzero 
%= \hejG\pvsig \pportw 
%= c_3 \hejG\pvsig \wrapBracks{\minvAB{\pvsig}{\pvmu} -
%\trAB{\hejG}{\vect{\gamma_2}}},
%$$ 
%and therefore 
%$\vect{\gamma_2} = \minvAB{\wrapParens{\qoform{\pvsig}{\hejG}}}{\hejG}\pvmu.$
%Thus
%$$
%\pportw = c_3\wrapBracks{\minvAB{\pvsig}{\pvmu} -
%\qiform{\wrapParens{\qoform{\pvsig}{\hejG}}}{\hejG}\pvmu}.
%$$
%Plugging this into the objective reduces the problem to the
%univariate optimization
%\begin{equation*}
%\max_{c_3 :\, c_3^2 \le \fracc{\Rbuj^2}{\psnrsqoptG{\hejG}}} 
%\sign{c_3} \psnroptG{\hejG} - \frac{\rfr}{\abs{c_3}\psnroptG{\hejG}},
%\end{equation*}
%where $\psnrsqoptG{\hejG} = \qiform{\pvsig}{\pvmu} - 
	%\qiform{\wrapParens{\qoform{\pvsig}{\hejG}}}{\wrapParens{\hejG\pvmu}}.$
%The optimum
%occurs for $c = \fracc{\Rbuj}{\psnroptG{\hejG}}$, moreover the optimum
%is unique when $\rfr > 0$.
%\end{proof}%UNFOLD
%%UNFOLD

%UNFOLD

%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Distribution under Gaussian returns}%FOLDUP

The goal of this section is to derive a variant of 
\theoremref{inv_distribution} for the case where \vreti follow a 
multivariate Gaussian distribution. First, 
assuming $\vreti\sim\normlaw{\pvmu,\pvsig}$, we can express the density
of \vreti, and of \svsm, in terms of \nlatf, \ssiz, and \pvsm.

\begin{lemma}%FOLDUP
Suppose $\vreti\sim\normlaw{\pvmu,\pvsig}$. Letting 
$\avreti = \asvec{1,\tr{\vreti}}$, and $\pvsm = \E{\ogram{\avreti}}$,
then the negative log likelihood of \vreti is
\begin{equation}
- \log\normpdf{\vreti}{\pvmu,\pvsig} = 
  c_{\nlatf} 
+ \half \logdet{\pvsm} 
+ \half \trace{\minv{\pvsm}\ogram{\avreti}},
\end{equation}
for the constant 
$c_{\nlatf} = -\half + \half[\nlatf]\log\wrapParens{2\pi}.$
\label{lemma:x_dist_gaussian}
\end{lemma}%UNFOLD
\begin{proof}%FOLDUP
By the block determinant formula,
\begin{equation*}
\det{\pvsm} 
= \det{1}\det{\wrapParens{\pvsig + \ogram{\pvmu}} - \pvmu 1^{-1} \tr{\pvmu}}
= \det{\pvsig}.
\end{equation*}
Note also that
\begin{equation*}
\qiform{\pvsig}{\wrapParens{\vreti - \pvmu}} = 
\qiform{\pvsm}{\avreti} - 1.
\end{equation*}
These relationships hold without assuming a particular distribution for 
\vreti. 

The density of \vreti is then
\begin{equation*}
\begin{split}
\normpdf{\vreti}{\pvmu,\pvsig} &= \frac{1}{\sqrt{\wrapParens{2\pi}^{\nlatf}\det{\pvsig}}} 
\longexp{-\half \qiform{\pvsig}{\wrapParens{\vreti - \pvmu}}},\\
 &= \frac{\detpow{\pvsig}{-\half}}{\wrapParens{2\pi}^{\nlatf/2}}
\longexp{-\half \wrapParens{\qiform{\pvsm}{\avreti} - 1}},\\
 &= \wrapParens{2\pi}^{-\nlatf/2} \detpow{\pvsm}{-\half}
\longexp{-\half \wrapParens{\qiform{\pvsm}{\avreti} - 1}},\\
 &= \wrapParens{2\pi}^{-\nlatf/2}
\longexp{\half - \half \logdet{\pvsm} - \half \trace{\minv{\pvsm}\ogram{\avreti}}},
%\therefore - \log\normpdf{\vreti}{\pvmu,\pvsig} &= 
	%- \half + \half[\nlatf]\log\wrapParens{2\pi}
%+ \half \logdet{\pvsm} 
%+ \half \trace{\minv{\pvsm}\ogram{\avreti}}.
\end{split}
\end{equation*}
and the result follows.
\end{proof}
%UNFOLD

\begin{lemma}%FOLDUP
Let $\vreti\sim\normlaw{\pvmu,\pvsig}$, 
$\avreti = \asvec{1,\tr{\vreti}}$, 
and $\pvsm = \E{\ogram{\avreti}}$. 
Given \ssiz \iid samples \vreti[i], let 
Let $\svsm = \oneby{\ssiz}\sum_i \ogram{\avreti[i]}$.
Then the density of \svsm is 
\begin{equation}
\FOOpdf{}{\svsm}{\pvsm} =
\longexp{c'_{\ssiz,\nlatf}}\frac{\det{\svsm}^{\half[\ssiz-\nlatf-2]}}{\det{\pvsm}^{\half[\ssiz]}}
\longexp{-\half[\ssiz]\trace{\minv{\pvsm}\svsm}},
\label{eqn:theta_dist_gaussian}
\end{equation}
for some $c'_{\ssiz,\nlatf}.$
\label{lemma:theta_dist_gaussian}
\end{lemma}%UNFOLD
\begin{proof}%FOLDUP
Let \amreti be the matrix
whose rows are the vectors \tr{\vreti[i]}. 
From \lemmaref{x_dist_gaussian}, and using linearity of the trace, 
the negative log density of \amreti is
\begin{equation*}
\begin{split}
- \log\normpdf{\amreti}{\pvsm} &=
  \ssiz c_{\nlatf} 
+ \half[\ssiz] \logdet{\pvsm} 
+ \half \trace{\minv{\pvsm}\gram{\amreti}},\\
\therefore
\frac{- 2\log\normpdf{\amreti}{\pvsm}}{\ssiz} &= 
  2 c_{\nlatf} 
+ \logdet{\pvsm} 
+ \trace{\minv{\pvsm}\svsm}.
\end{split}
\end{equation*}

By Lemma (5.1.1) of Press \cite{press2012applied}, this can be expressed
as a density on \svsm:
\begin{equation*}
\begin{split}
\frac{- 2\log\FOOpdf{}{\svsm}{\pvsm}}{\ssiz} 
&= \frac{- 2\log\normpdf{\amreti}{\pvsm}}{\ssiz}
	-\frac{2}{\ssiz}\wrapParens{\half[\ssiz-\nlatf-2]\logdet{\svsm}}\\
&\phantom{=}\,
	-\frac{2}{\ssiz}\wrapParens{\half[\nlatf+1]\wrapParens{\ssiz - \half[\nlatf]} \log\pi -
	\sum_{j=1}^{\nlatf+1} \log\funcit{\Gamma}{\half[\ssiz +1-j]}},\\
&= \wrapBracks{2c_{\nlatf} - \frac{\nlatf+1}{\ssiz}\wrapParens{\ssiz - \half[\nlatf]} \log\pi 
	- \frac{2}{\ssiz} \sum_{j=1}^{\nlatf+1}
	\log\funcit{\Gamma}{\half[\ssiz +1-j]}}\\
&\phantom{=}\,
	+ \logdet{\pvsm} - \frac{\ssiz-\nlatf-2}{\ssiz}\logdet{\svsm}
	+ \trace{\minv{\pvsm}\svsm},\\
&= c'_{\ssiz,\nlatf} 
	- \log\frac{\det{\svsm}^{\frac{\ssiz-\nlatf-2}{\ssiz}}}{\det{\pvsm}}
	+ \trace{\minv{\pvsm}\svsm},
\end{split}
\end{equation*}
where $c'_{\ssiz,\nlatf}$ is the term in brackets on the third line.
Factoring out $\fracc{-2}{\ssiz}$ and taking an exponent gives the
result.
\end{proof}%UNFOLD
\begin{corollary}
The random variable $\ssiz\svsm$ has the same density, up to a constant 
in \nlatf and \ssiz, as a 
$\nlatf+1$-dimensional Wishart random variable with \ssiz degrees of freedom
and scale matrix \pvsm. Thus $\ssiz\svsm$ is a \emph{conditional} Wishart,
conditional on $\svsm_{1,1} = 1$.  \cite{press2012applied,anderson2003introduction}
%conditional on $\svsm_{\nlatf+1,\nlatf+1} = 1$.  % for the other form.
\end{corollary}

\subsection{Maximum likelihood estimator}%FOLDUP

%The maximum likelihood estimator of \ichol{\pvsm} is found by 
%taking the derivative of the (log) likelihood and finding a root.
%That is,
%\begin{equation*}
%\begin{split}
%0 &= \drbydr{\log\FOOpdf{}{\svsm}{\pvsm}}{\ichol{\pvsm}},\\
  %&= -2 \trminv{\wrapParens{\ichol{\pvsm[m]}}} + \ichol{\pvsm[m]}2\svsm,
%\end{split}
%\end{equation*}
%leaning heavily on the Matrix Cookbook for 
%the derivatives. \cite{petersen2012matrix}
%This is solved by $\ichol{\pvsm[m]} = \ichol{\svsm}$, and thus the
%na\"{i}ve sample estimate is the MLE.
\providecommand{\txtMLE}{\mbox{MLE}}

\begin{lemma}%FOLDUP
\svsm is the maximum likelihood estimator of \pvsm. \label{lemma:theta_mle}
\end{lemma}%UNFOLD
\begin{proof}%FOLDUP
The maximum likelihood estimator of \pvsm would be found by 
taking the derivative of the (log) likelihood, given by 
\lemmaref{theta_dist_gaussian}, with respect to
\pvsm and finding a root. However, the derivative of log likelihood
with respect to \pvsm is unpleasant, so instead take the derivative with
respect to \minv{\pvsm}:
%with respect to \pvsm is mildly unpleasant:
%\begin{equation}
%\begin{split}
%\drbydr{\log\FOOpdf{}{\svsm}{\pvsm}}{\pvsm} 
	%&= - \half[\ssiz]\drbydr{\logdet{\pvsm}}{\pvsm} 
	%- \half[\ssiz]\drbydr{\trace{\minv{\pvsm}\svsm}}{\pvsm},\\
	%&= - \half[\ssiz]\minv{\pvsm}
	%+ \half[\ssiz]\minv{\pvsm}\svsm\minv{\pvsm},
%\end{split}
%\label{eqn:mle_deadend}
%\end{equation}
%However, the derivative with respect to \minv{\pvsm} is a bit
%simpler:
\begin{equation}
\begin{split}
\drbydr{\log\FOOpdf{}{\svsm}{\pvsm}}{\minv{\pvsm}} 
	&= \half[\ssiz]\drbydr{\logdet{\minv{\pvsm}}}{\minv{\pvsm}} 
	- \half[\ssiz]\drbydr{\trace{\minv{\pvsm}\svsm}}{\minv{\pvsm}},\\
  &= \half[\ssiz]\wrapParens{\pvsm - \svsm},
\end{split}
\end{equation}
where the matrix calculus follows from the 
Matrix Cookbook.  \cite{petersen2012matrix}
\nocite{magnus1999matrix,petersen2012matrix}
Thus the likelihood is maximized by 
$\pvsm[\txtMLE] = \svsm$.
\end{proof}%UNFOLD

%Since $\pvsm[\txtMLE] = \svsm$, the log likelihood of the MLE is 
%\begin{equation}
%\begin{split}
%\log\FOOlik{}{\svsm}{\pvsm[\txtMLE]} 
 %&= - \half[\ssiz] c'_{\ssiz,\nlatf} - \half[\ssiz] \logdet{\pvsm[\txtMLE]} +
 %\half[\ssiz-\nlatf-2]\logdet{\svsm}\\
%&\phantom{=}\,+ \trace{\minv{\pvsm[\txtMLE]}\svsm},\\
 %&= -\half[\ssiz] c'_{\ssiz,\nlatf} 
 %- \half[\nlatf+2]\logdet{\svsm} + \wrapParens{\nlatf+1}.
%\end{split}
%\end{equation}

\providecommand{\FishI}[1][{}]{\mathSUB{\mathcal{I}}{#1}}
\providecommand{\FishIf}[2][{}]{\funcit{\mathSUB{\mathcal{I}}{#1}}{#2}}
\providecommand{\qfElim}[1]{\qform{#1}{\Elim}}
\providecommand{\qoElim}[1]{\qoform{#1}{\Elim}}
\providecommand{\qoElim}[1]{\qoform{#1}{\Elim}}

\begin{lemma}%FOLDUP
The Fisher Information of $\minv{\pvsm}$ is
\begin{equation}
\FishIf[\ssiz]{\minv{\pvsm}} = \half[\ssiz]
\qoElim{\wrapParens{\AkronA{\pvsm}}}.
\label{eqn:itheta_fish_inf_eqn}
\end{equation}
\label{lemma:itheta_fish_inf}
\end{lemma}%UNFOLD
\begin{proof}%FOLDUP
First compute
the Hessian of $\log\FOOpdf{}{\svsm}{\pvsm}$ with
respect to $\fvech{\minv{\pvsm}}$. From the proof of
\lemmaref{theta_mle}, 
\begin{equation*}
\drbydr{\log\FOOpdf{}{\svsm}{\pvsm}}{\minv{\pvsm}}
 = \half[\ssiz]\wrapParens{\pvsm - \svsm}.
\end{equation*}
So
\begin{equation*}
\begin{split}
\drbydr[2]{\log\FOOpdf{}{\svsm}{\pvsm}}{\minv{\pvsm}}
 & \defeq
\drbydr{\fvech{\drbydr{\log\FOOpdf{}{\svsm}{\pvsm}}{\minv{\pvsm}}}}{\fvech{\minv{\pvsm}}},\\
 &= - \half[\ssiz] \qoElim{\wrapParens{\AkronA{\pvsm}}},
\end{split}
\end{equation*}
%via \lemmaref{deriv_vech_matrix_inverse}, where, again, \Elim is the 
%`Elimination Matrix.' \cite{magnus1980elimination}
via \lemmaref{deriv_vech_matrix_inverse}. 
Since the Fisher Information is negative the expected value of this 
Hessian, the result follows. \cite{pawitanIAL}
\end{proof}%UNFOLD
\begin{corollary}%FOLDUP
By change of variables, and again using \lemmaref{deriv_vech_matrix_inverse}, 
the Fisher Information with respect to \pvsm is
\begin{equation}
\FishIf[\ssiz]{\pvsm} = \half[\ssiz]
\qform{\wrapParens{\qoElim{\wrapParens{\AkronA{\pvsm}}}}}{\wrapBracks{\qoElim{\wrapParens{\AkronA{\minv{\pvsm}}}}}}.
\end{equation}
\end{corollary}%UNFOLD

Thus the analogue of \theoremref{inv_distribution} for Gaussian returns
is given by the following theorem.
\begin{theorem}%FOLDUP
Let \svsm be the unbiased sample estimate of 
\pvsm, based on \ssiz \iid samples of \vreti, assumed multivariate
Gaussian.
Then, asymptotically in \ssiz, 
\begin{equation}
\sqrt{\ssiz}\wrapParens{\fvech{\minv{\svsm}} - \fvech{\minv{\pvsm}}} 
\rightsquigarrow 
\normlaw{0,2 \minv{\wrapParens{\qoform{\wrapParens{\AkronA{\pvsm}}}{\Elim}}}}.
\label{eqn:mvclt_isvsm_gaussian}
\end{equation}
\label{theorem:inv_distribution_gaussian}
\end{theorem}%UNFOLD
\begin{proof}%FOLDUP
Under `the appropriate regularity conditions,'
\cite{wasserman2004all,pawitanIAL}
\begin{equation}
\label{eqn:converge_theta}
\wrapParens{\fvech{\minv{\svsm}} - \fvech{\minv{\pvsm}}} 
\rightsquigarrow 
\normlaw{0,\minv{\wrapParens{\FishIf[\ssiz]{\minv{\pvsm}}}}},
\end{equation}
and the result follows from 
\lemmaref{itheta_fish_inf}.
\end{proof}%UNFOLD

The `plug-in' estimator of the covariance substitutes in \svsm for
\pvsm in the right hand side of \eqnref{mvclt_isvsm_gaussian}.
%UNFOLD

\subsection{Likelihood ratio test on \txtMP}%FOLDUP

\providecommand{\lrtA}[1][i]{\mathSUB{\Mtx{A}}{#1}}
\providecommand{\lrta}[1][i]{\mathSUB{a}{#1}}

%This section is based on Dempster's ``Covariance Selection''. \cite{dempster1972}

For some conformable symmetric matrices \lrtA, and given scalars \lrta, 
consider the null hypothesis
\begin{equation}
H_0: \trace{\lrtA[i]\minv{\pvsm}} = \lrta[i],\,i=1,\ldots,m.
\label{eqn:lrt_null_back}
\end{equation}
The constraints have to be sensible. 
For example, they cannot
violate the positive definiteness of 
\minv{\pvsm}, \etc Without loss of generality, we can assume
that the \lrtA[i] are symmetric, since \pvsm is symmetric, and for
symmetric \Mtx{G} and square \Mtx{H}, 
$\trace{\Mtx{G}\Mtx{H}} = \trace{\Mtx{G}\half\wrapParens{\Mtx{H} +
\tr{\Mtx{H}}}}$, and so we could replace any non-symmetric \lrtA[i] with
$\half\wrapParens{\lrtA[i] + \tr{\lrtA[i]}}$.

Employing the Lagrange multiplier technique, the maximum likelihood
estimator under the null hypothesis, call it \pvsm[0], solves the
following equation
\begin{equation}
\begin{split}
0 &= \drbydr{\log\FOOpdf{}{\svsm}{\pvsm}}{\minv{\pvsm}}
- \sum_i \lambda_i
\drbydr{\trace{\lrtA[i]\minv{\pvsm}}}{\minv{\pvsm}},\notag \\
&= - \pvsm[0] + \svsm - \sum_i \lambda_i \lrtA[i],\notag.
\end{split}
\end{equation}
Thus the MLE under the null is
\begin{equation}
\pvsm[0] = \svsm - \sum_i \lambda_i \lrtA[i].
\label{eqn:lrt_mle_soln}
\end{equation}
The maximum likelihood estimator under the constraints has to be
found numerically by solving for the $\lambda_i$, subject
to the constraints in \eqnref{lrt_null_back}.

This framework slightly generalizes Dempster's 
``Covariance Selection,'' \cite{dempster1972} which
reduces to the case where each \lrta[i] is zero, and
each \lrtA[i] is a matrix of all zeros except two (symmetric) ones 
somewhere in the lower right $\nlatf \times \nlatf$ sub matrix. In
all other respects, however, the solution here follows Dempster.

\providecommand{\vitrlam}[2]{\vectUL{\lambda}{\wrapNeParens{#1}}{#2}}
\providecommand{\sitrlam}[2]{\mathUL{\lambda}{\wrapNeParens{#1}}{#2}}
\providecommand{\vitrerr}[2]{\vectUL{\epsilon}{\wrapNeParens{#1}}{#2}}
\providecommand{\sitrerr}[2]{\mathUL{\epsilon}{\wrapNeParens{#1}}{#2}}

An iterative technique for finding the MLE based on a Newton step 
would proceed as follow.  \cite{nocedal2006numerical} 
Let \vitrlam{0}{} be some initial estimate of the
vector of $\lambda_i$. (A good initial estimate can likely be had 
by abusing the asymptotic normality 
result from \subsecref{dist_markoport}.)
The residual of the \kth{k} estimate, \vitrlam{k}{} is
\begin{equation}
\vitrerr{k}{i} \defeq 
\trace{\lrtA[i]\minv{\wrapBracks{\svsm - \sum_j \sitrlam{k}{j} \lrtA[j]}}} - \lrta[i].
\end{equation}
The Jacobian of this residual with respect to the \kth{l} element of \vitrlam{k}
is
\begin{equation}
\begin{split}
\drbydr{\vitrerr{k}{i}}{\sitrlam{k}{l}} &= 
\trace{\lrtA[i]\minv{\wrapBracks{\svsm - \sum_j \sitrlam{k}{j} \lrtA[j]}}
\lrtA[l] \minv{\wrapBracks{\svsm - \sum_j \sitrlam{k}{j} \lrtA[j]}}},\\
&= \tr{\fvec{\lrtA[i]}} \wrapParens{\AkronA{\minv{\wrapBracks{\svsm - \sum_j
\sitrlam{k}{j}\lrtA[j]}}}} \fvec{\lrtA[l]}.
\end{split}
\end{equation}

Newton's method is then the iterative scheme
\begin{equation}
\vitrlam{k+1}{} \leftarrow \vitrlam{k}{} -
\minv{\wrapParens{\drbydr{\vitrerr{k}{}}{\vitrlam{k}{}}}} \vitrerr{k}.
\end{equation}
%Note there is no reason one must use a Newton scheme to maximize the
%likelihood. High quality optimization routines (\eg BFGS) which require
%only the gradient to be 
%There is no reason to restrict oneself to a Newton solver. High quality
%optimization routines which require ...
%\cite{nocedal2006numerical} 

When (if?) the iterative scheme converges on the optimum, plugging in
\vitrlam{k}{} into \eqnref{lrt_mle_soln} gives the MLE under the null.
The likelihood ratio test statistic is 
\begin{equation}
\begin{split}
-2\log\Lambda &\defeq
-2\log\wrapParens{\frac{\FOOlik{}{\svsm}{\pvsm[0]}}{\FOOlik{}{\svsm}{\pvsm[\mbox{unrestricted }\txtMLE]}}},\\
&= \ssiz\wrapParens{\logdet{\pvsm[0]\minv{\svsm}} + 
\trace{\wrapBracks{\minv{\pvsm[0]} - \minv{\svsm}}\svsm}},\\
&= \ssiz\wrapParens{\logdet{\pvsm[0]\minv{\svsm}} + 
\trace{\minv{\pvsm[0]}\svsm} - \wrapBracks{\nlatf + 1}},
\end{split}
\label{eqn:wilks_lambda_def}
\end{equation}
using the fact that \svsm is the unrestricted MLE, per 
\lemmaref{theta_mle}.
By Wilks' Theorem, under the null 
hypothesis, $-2\log\Lambda$ is, asymptotically in \ssiz, distributed as 
a chi-square with $m$ degrees of freedom. \cite{wilkstheorem1938}

%UNFOLD
%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Examples}%FOLDUP

%Empirically, the marginal Wald test for zero weighting in the 
%\txtMP based on this approximation are nearly
%identical to the \tstat-statistics produced by the procedure
%of Britten-Jones, as shown below. \cite{BrittenJones1999} 

%<<'me_vs_bjones',echo=TRUE>>=
%nday <- 1024
%nstk <- 5

%# under the null: all returns are zero mean;
%set.seed(as.integer(charToRaw("7fbb2a84-aa4c-4977-8301-539e48355a35")))
%rets <- matrix(rnorm(nday * nstk),nrow=nday)

%# t-stat via Britten-Jones procedure
%bjones.ts <- function(rets) {
	%ones.vec <- matrix(1,nrow=dim(rets)[1],ncol=1)
	%bjones.mod <- lm(ones.vec ~ rets - 1)
	%bjones.sum <- summary(bjones.mod)
	%retval <- bjones.sum$coefficients[,3]
%}
%# wald stat via inverse second moment trick
%ism.ws <- function(rets,...) {
%# flipping the sign on returns is idiomatic,
	%asymv <- ism_vcov(- as.matrix(rets),...)
	%asymv.mu <- asymv$mu[1:asymv$p]
	%asymv.Sg <- asymv$Ohat[1:asymv$p,1:asymv$p]
	%retval <- asymv.mu / sqrt(diag(asymv.Sg))
%}

%bjones.tstat <- bjones.ts(rets)
%ism.wald <- ism.ws(rets)

%# compare them:
%print(bjones.tstat)
%print(ism.wald)

%# repeat under the alternative;
%set.seed(as.integer(charToRaw("a5f17b28-436b-4d01-a883-85b3e5b7c218")))
%zero.rets <- t(matrix(rnorm(nday * nstk),nrow=nday))
%mu.vals <- (1/sqrt(253)) * seq(-1,1,length.out=nstk) 
%rets <- t(zero.rets + mu.vals)

%bjones.tstat <- bjones.ts(rets)
%ism.wald <- ism.ws(rets)

%# compare them:
%print(bjones.tstat)
%print(ism.wald)
%@

%%<<'test_FF3',echo=TRUE>>=
%%ff3 <- read.csv('http://www.quandl.com/api/v1/datasets/KFRENCH/FACTORS_M.csv?&trim_start=1926-07-31&trim_end=2013-10-31&sort_order=desc', colClasses=c('Month'='Date'))

%%require(sandwich)

%%rfr <- ff3[,'RF']
%%hml <- ff3[,'HML'] - rfr
%%smb <- ff3[,'SMB'] - rfr

%%ff.ret <- cbind(mkt=ff3[,'Mkt.RF'],hml,smb)

%%my.ws <- ism.ws(ff.ret)
%%my.ws2 <- ism.ws(ff.ret,vcov.func=vcovHAC)
%%my.bt <- bjones.ts(ff.ret)

%%# conditionally?
%%blahm <- cbind(-ff.ret[(1:(dim(ff.ret)[1]-1)),],ff.ret[(2:dim(ff.ret)[1]),])
%%# futz from here ... 
%%XX <- ism_vcov(blahm)
%%@

%% 2FIX: add examples here ... 
%%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% bibliography%FOLDUP
\nocite{markowitz1952portfolio,markowitz1999early,markowitz2012foundations}
%\bibliographystyle{jss}
%\bibliographystyle{siam}
%\bibliographystyle{ieeetr}
\bibliographystyle{plainnat}
%\bibliographystyle{acm}
%\bibliography{SharpeR,rauto}
\bibliography{AsymptoticMarkowitz}
%UNFOLD

\end{document}
%for vim modeline: (do not edit)
% vim:fdm=marker:fmr=FOLDUP,UNFOLD:cms=%%s:syn=rnoweb:ft=rnoweb:nu
