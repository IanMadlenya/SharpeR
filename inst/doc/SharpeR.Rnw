%\VignetteEngine{knitr}
%\VignetteIndexEntry{Statistics of the Sharpe ratio}
%\VignetteKeywords{Finance}
%\VignettePackage{SharpeR}
\documentclass[10pt,a4paper,english]{article}

\usepackage{url}
\usepackage{amsmath}
%\usepackage[authoryear]{natbib}

\makeatletter

\makeatother

\input{sharpe_shortcuts.tex}
\providecommand{\rcode}[1]{\texttt{#1}}
<<'preamble', include=FALSE, warning=FALSE, message=FALSE>>=
library(knitr)

# set the knitr options ... for everyone!
# if you unset this, then vignette build bonks. oh, joy.
#opts_knit$set(progress=TRUE)

# for a package vignette, you do want to echo.
# opts_chunk$set(echo=FALSE,warning=FALSE,message=FALSE)
opts_chunk$set(warning=FALSE,message=FALSE)
#opts_chunk$set(results="asis")
opts_chunk$set(cache=TRUE,cache.path="cache/")
opts_chunk$set(fig.path="figure/",dev=c("pdf","cairo_ps"))
#opts_knit$set(eps=TRUE)

compile.time <- Sys.time()

# compiler flags!

# not used yet
LONG.FORM <- FALSE

library(quantmod)
options("getSymbols.warning4.0"=FALSE)
@

\begin{document}

\title{SharpeR}


\author{Steven E. Pav}

\maketitle

\begin{abstract}%FOLDUP
The SharpeR package provides basic functionality for testing significance of
the \txtSR of a series of returns, and of the Markowitz portfolio on a 
number of possibly correlated assets.\cite{Sharpe:1966} The goal of the package
is to make it simple to estimate profitability (in terms of risk-adjusted
returns) of strategies or asset streams.
\end{abstract}%UNFOLD

\section{The \txtSR}%FOLDUP
Sharpe defined the 'reward to variability ratio', now known as the 
'\txtSR', as the sample statistic
$$
\ssr = \frac{\smu}{\ssig},
$$
where \smu is the sample mean, and \ssig is the sample standard deviation.
\cite{Sharpe:1966} The
\txtSR was later redefined to include a 'risk-free' or 'disastrous rate
of return': $\ssr = \wrapParens{\smu - \rfr}/\ssig.$ 

It is little appreciated in quantitative finance that the \txtSR is identical 
to the sample statistic proposed by Gosset in 1908 to test for zero mean
when the variance is unknown.  \cite{student08ttest} The `\tstat-test' we know
today, which includes an adjustment for sample size, was formulated later
by Fisher.  \cite{Fisher1925} Knowing that the \txtSR is related to the 
\tstat-statistic provides a `natural arbitrage,' since the latter has 
been extensively studied.  Many of the interesting properties of the
\tstat-statistic can be translated to properties about the \txtSR. 

Also little appreciated is that the multivariate analogue of the \tstat-statistic,
Hotelling's \Tstat, is related to the Markowitz portfolio. Consider the
following portfolio optimization problem:
$$
\max_{\sportw : \qform{\svsig}{\sportw} \le R^2} 
\frac{\trAB{\sportw}{\svmu} - \rfr}{\sqrt{\qform{\svsig}{\sportw}}},
$$
where \svmu, \svsig are the sample mean vector and covariance matrix, \rfr is
the risk-free rate, and $R$ is a cap on portfolio 'risk' as estimated by
\svsig. (Note this differs from the traditional definition of the problem which
imposes a 'self-financing constraint' which does not actually bound portfolio
weights.)  The solution to this problem is
$$
\sportwopt = \frac{R}{\sqrt{\qform{\minv{\svsig}}{\svmu}}} \minv{\svsig}\svmu.
$$
The \txtSR of this portfolio is 
$$
\ssropt 
= \frac{\trAB{\sportwopt}{\svmu} - \rfr}{\sqrt{\qform{\svsig}{\sportwopt}}} 
= \sqrt{\qform{\minv{\svsig}}{\svmu}} - \frac{\rfr}{R}
= \sqrt{\Tstat / \ssiz} - \frac{\rfr}{R},
$$
where \Tstat is Hotelling's statistic, and \ssiz is the number of independent 
observations (\eg `days') used to construct \svmu. The term $\rfr / R$ is a
deterministic `drag' term that merely shifts the location of \ssropt, and so
we can (mostly) ignore it when testing significance of \ssropt.

Under the (typically indefensible) assumptions that the returns are generated
\iid from a normal distribution (multivariate normal in the case of the
portfolio problem), the distributions of \ssr and \ssrsqopt are known, and
depend on the sample size and the population analogues, \psr and \psnrsqopt.
In particular, they are distributed as rescaled non-central \tlaw{} and \flaw{}
distributions. Under these assumptions on the generating processes, we can
perform inference on the population analogues using the sample statistics.

The importance of each of these assumptions (\viz homoskedasticity,
independence, normality, \etc) can and should be checked. It should be noted
that this package is distributed without any warranty
of any kind, and in no way should any analysis performed with this package be
interpreted as implicit investment advice by the author(s).

The units of \smu are `returns per time,' while those of \ssig are `returns per
square root time.' Consequently, the units of \ssr are `per square root time.'
Typically the \txtSR is quoted in `annualized' terms, \ie \yrto{-1/2}, but the
units are omitted. I believe that units should be included as it avoids
ambiguity, and simplifies conversions.

There is no clear standard whether arithmetic or geometric returns should be
used in the computation of the \txtSR. Since arithmetic returns are always
greater than the equivalent geometric returns, one would suspect that
arithmetic returns are \emph{always} used when advertising products. However, I
suspect that geometric returns are more frequently used in the analysis of
strategies. Geometric returns have the attractive property of being `additive',
meaning that the geometric return of a period is the sum of those of
subperiods, and thus the sign of the arithmetic mean of some geometric returns
indicates whether the final value of a strategy is greater than the initial
value. Oddly, the arithmetic mean of arithmetic returns does not share this
property. 

On the other hand, arithmetic returns are indeed additive 
\emph{contemporaneously}: if \vreti is the vector of arithmetic returns of
several stocks, and \sportw is the dollar proportional allocation into those
stocks at the start of the period, then \trAB{\vreti}{\sportw} is the
arithmetic return of the portfolio over that period. This holds even when the
portfolio holds some stocks `short.'  Often this portfolio accounting is
misapplied to geometric returns without even an appeal to Taylor's theorem.

%UNFOLD

\section{Basic Usage}%FOLDUP

An \rcode{sr} object encapsulates one or more \txtSR statistics, along with the
degrees of freedom, the rescaling to a \tstat statistic, and the annualization
and units information. One can simply stuff this information into an \rcode{sr} 
object, but it is more straightforward to allow \rcode{as.sr} to compute the
\txtSR for you. 

<<'babysteps'>>=
library(SharpeR)
# suppose you computed the Sharpe of your strategy to
# be 1.3 / sqrt(yr), based on 1200 daily observations.
# store them as follows:
my.sr <- sr(sr=1.3,df=1200-1,ope=252,epoch="yr")
print(my.sr)
# multiple strategies can be tracked as well.
# one can attach names to them.
srstats <- c(0.5,1.2,0.6)
dim(srstats) <- c(3,1)
rownames(srstats) <- c("strat. A","strat. B","benchmark")
my.sr <- sr(srstats,df=1200-1,ope=252,epoch="yr")
print(my.sr)
@

Throughout, \rcode{ope} stands for `Observations Per Epoch', and is the
(average) number of returns observed per the annualizatin period, called
the \rcode{epoch}.  At the moment there is not much hand holding regarding
these parameters: no checking is performed for sane values.

The \rcode{as.sr} method will compute the \txtSR for you, from numeric,
\rcode{data.frame}, \rcode{xts} or \rcode{lm} objects. In the latter case, it
is assumed one is performing an attribution model, and the statistic of
interest is the fit of the \rcode{(Intercept)} term divided by the residual
standard deviation. Here are some examples:

<<'showoff'>>=
set.seed(as.integer(charToRaw("set the seed")))
# Sharpe's 'model': just given a bunch of returns.
returns <- rnorm(253*8,mean=3e-4,sd=1e-2)
asr <- as.sr(returns,ope=253,epoch="yr")
print(asr)
# a data frame with a single strategy
asr <- as.sr(data.frame(my.strategy=returns),ope=253,epoch="yr")
print(asr)
@

When a \rcode{data.frame} with multiple columns is given, the \txtSR of each is
computed, and they are all stored:
<<'more_data_frame'>>=
# a data frame with multiple strategies
asr <- as.sr(data.frame(strat1=rnorm(253*8),strat2=rnorm(253*8,mean=4e-4,sd=1e-2)),
	ope=253,epoch="yr")
print(asr)
@

Here is an example using \rcode{xts} objects. In this case, if the \rcode{ope}
is not given, it is inferred from the time marks of the input object.
<<'some_stocks'>>=
library(quantmod)
get.ret <- function(sym,...) {
	OHLCV <- getSymbols(sym,auto.assign=FALSE,...)
	adj.names <- paste(c(sym,"Adjusted"),collapse=".",sep="")
	lrets <- diff(log(OHLCV[,adj.names]))
	#chop first
	lrets[-1,]
}
get.rets <- function(syms,...) { some.rets <- do.call("cbind",lapply(syms,get.ret,...)) }
some.rets <- get.rets(c("AAPL","IBM","A","C"),from="2001-01-01")
print(as.sr(some.rets))
@

The annualization of an \rcode{sr} object can be changed with the
\rcode{reannualize} method. The name of the epoch and the observation rate can
both be changed.

<<'reannualize'>>=
some.rets <- get.rets(c("XOM"),from="2001-01-01")
yearly <- as.sr(some.rets)
monthly <- reannualize(yearly,new.ope=21,new.epoch="mo.")
print(yearly)
print(monthly)
@

%UNFOLD
\subsection{Attribution Models}%FOLDUP

When an object of class \rcode{lm} is given to \rcode{as.sr}, the 
fit \rcode{(Intercept)} term is divided by the residual volatility to compute
something like the \txtSR. In terms of Null Hypothesis Significance Testing,
nothing is gained by summarizing the \rcode{sr} object instead of the
\rcode{lm} object. However, confidence intervals on the \txtSR are quoted in
the more natural units of reward to variability, and in annualized terms (or
whatever the epoch is.)

As an example, here I perform

<<'BRKB'>>=
some.rets <- get.rets(c("BRK.B","SPY"),from="1996-05-09")
# make them monthly.
mo.rets <- do.call(cbind,lapply(exp(cumsum(some.rets)),function(x) {
	retv <- monthlyReturn(x)
	colnames(retv) <- colnames(x)
	return(retv)
}))
# first look at both of these:
both.sr <- as.sr(some.rets)
both.sr <- as.sr(mo.rets)
print(both.sr)
print(confint(both.sr))
# compute Sharpe of idiosyncratic term under CAPM
linmod <- lm(BRK.B.Adjusted ~ SPY.Adjusted,data=mo.rets)
CAPM.sr <- as.sr(linmod,ope=both.sr$ope,epoch="yr")
print(summary(linmod))
print(CAPM.sr)
print(confint(linmod,'(Intercept)'))
print(confint(CAPM.sr))
@
%UNFOLD

\subsection{Attribution Models}

\subsection{Distribution Functions}

There are \rcode{dpqr} functions for the \txtSR distribution, as well as the
`optimal \txtSR' distribution. These are merely rescaled non-central 
\tstat and \Fstat distributions, provided for convenience, and for testing
purposes.

\nocite{CambridgeJournals:4493808,lo2002,Lecoutre2007,Johnson:1940}

%\bibliographystyle{jss}
\bibliographystyle{acm}
\bibliography{SharpeR}

\end{document}
%for vim modeline: (do not edit)
% vim:fdm=marker:fmr=FOLDUP,UNFOLD:cms=%%s:syn=rnoweb:ft=rnoweb
